//! Types for the MegaDB Configurator — K8s cluster hardware deployment.
//!
//! Enables users to configure MegaDB K8s clusters with specific
//! hardware accelerators across AWS, GCP, and Azure:
//! - Cloud provider + instance type selection
//! - GPU/FPGA/NPU/TPU accelerator specification
//! - Live K8s node hardware availability detection
//!
//! Mirrors MegaDB CRD types:
//! - `megadb-k8s/src/crd.rs` — MegaDBSpec, AcceleratorSpec, GpuResourceSpec, WorkerGroupSpec
//! - `megadb-core/src/config.rs` — AcceleratorConfig, GpuConfig, FpgaConfig, NpuConfig

use serde::{Deserialize, Serialize};

// ─── Cloud Provider & Instance Catalog ──────────────────────────────────────

/// Cloud provider for the K8s cluster.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum CloudProvider {
    Aws,
    Gcp,
    Azure,
    OnPrem,
}

impl CloudProvider {
    pub fn label(&self) -> &'static str {
        match self {
            Self::Aws => "AWS",
            Self::Gcp => "GCP",
            Self::Azure => "Azure",
            Self::OnPrem => "On-Premises",
        }
    }

    pub fn all() -> &'static [CloudProvider] {
        &[Self::Aws, Self::Gcp, Self::Azure, Self::OnPrem]
    }
}

/// Type of hardware accelerator available on an instance.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum AcceleratorType {
    Gpu,
    Fpga,
    Npu,
    Tpu,
    None,
}

impl AcceleratorType {
    pub fn label(&self) -> &'static str {
        match self {
            Self::Gpu => "GPU",
            Self::Fpga => "FPGA",
            Self::Npu => "NPU",
            Self::Tpu => "TPU",
            Self::None => "CPU Only",
        }
    }

    pub fn badge(&self) -> &'static str {
        match self {
            Self::Gpu => "GPU",
            Self::Fpga => "FPGA",
            Self::Npu => "NPU",
            Self::Tpu => "TPU",
            Self::None => "CPU",
        }
    }
}

/// An instance family from a cloud provider (e.g., AWS p5, GCP a3).
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InstanceFamily {
    pub provider: CloudProvider,
    pub family: String,
    pub description: String,
    pub accelerator_type: AcceleratorType,
    pub instance_types: Vec<InstanceType>,
}

/// A specific instance type with hardware specifications.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InstanceType {
    pub name: String,
    pub vcpus: u32,
    pub memory_gb: u32,
    pub accelerator: Option<AcceleratorDetail>,
    pub network_gbps: Option<f64>,
    pub price_per_hour_usd: Option<f64>,
    pub k8s_resource_name: Option<String>,
}

/// Hardware accelerator details for an instance type.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AcceleratorDetail {
    pub accelerator_type: AcceleratorType,
    pub device_name: String,
    pub count: u32,
    pub memory_gb: Option<u32>,
    pub compute_capability: Option<String>,
    pub supports_mig: bool,
    pub mig_profiles: Vec<String>,
}

// ─── K8s Cluster Hardware Detection ─────────────────────────────────────────

/// Detected hardware on a K8s node.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NodeHardware {
    pub node_name: String,
    pub provider_instance_type: Option<String>,
    pub cpu_allocatable: String,
    pub memory_allocatable_bytes: u64,
    pub accelerators: Vec<DetectedAccelerator>,
    pub labels: Vec<(String, String)>,
}

/// A hardware accelerator detected on a K8s node.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DetectedAccelerator {
    pub resource_name: String,
    pub accelerator_type: AcceleratorType,
    pub allocatable: u32,
    pub allocated: u32,
    pub device_name: Option<String>,
}

/// Cluster-wide hardware availability summary.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct ClusterHardwareStatus {
    pub detected_provider: Option<CloudProvider>,
    pub nodes: Vec<NodeHardware>,
    pub total_gpus: u32,
    pub total_fpgas: u32,
    pub total_npus: u32,
    pub total_tpus: u32,
    pub gpu_available: u32,
    pub fpga_available: u32,
    pub npu_available: u32,
    pub tpu_available: u32,
    pub k8s_connected: bool,
    pub error: Option<String>,
}

// ─── Deployment Configuration ───────────────────────────────────────────────

/// MegaDB deployment configuration generated by the configurator.
///
/// Maps to the MegaDB CRD `MegaDBSpec` (megadb-k8s/src/crd.rs).
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DeploymentConfig {
    pub cluster_name: String,
    pub provider: CloudProvider,
    pub cpu_worker_pool: WorkerPoolConfig,
    pub gpu_worker_pool: Option<WorkerPoolConfig>,
    pub fpga_worker_pool: Option<WorkerPoolConfig>,
    pub npu_worker_pool: Option<WorkerPoolConfig>,
    pub storage_backend: StorageBackend,
    pub keda_enabled: bool,
    pub estimated_monthly_cost_usd: f64,
}

/// Worker pool configuration for a specific node type.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WorkerPoolConfig {
    pub instance_type: String,
    pub replicas: i32,
    pub min_replicas: i32,
    pub max_replicas: i32,
    pub accelerator: Option<PoolAccelerator>,
    pub node_selector: Vec<(String, String)>,
    pub tolerations: Vec<TolerationConfig>,
}

/// Accelerator assignment for a worker pool.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PoolAccelerator {
    pub accelerator_type: AcceleratorType,
    pub k8s_resource_name: String,
    pub count_per_pod: u32,
    pub device_name: String,
    pub mig_profile: Option<String>,
}

/// Toleration for accelerator node taints.
///
/// Mirrors `TolerationSpec` from megadb-k8s/src/crd.rs.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TolerationConfig {
    pub key: String,
    pub operator: String,
    pub value: Option<String>,
    pub effect: String,
}

/// Object store backend for OLAP data.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum StorageBackend {
    S3,
    Gcs,
    AzureBlob,
    Local,
}

impl StorageBackend {
    pub fn label(&self) -> &'static str {
        match self {
            Self::S3 => "AWS S3",
            Self::Gcs => "Google Cloud Storage",
            Self::AzureBlob => "Azure Blob Storage",
            Self::Local => "Local Storage",
        }
    }

    pub fn default_for(provider: CloudProvider) -> Self {
        match provider {
            CloudProvider::Aws => Self::S3,
            CloudProvider::Gcp => Self::Gcs,
            CloudProvider::Azure => Self::AzureBlob,
            CloudProvider::OnPrem => Self::Local,
        }
    }
}

// ─── Instance Catalog Builder ───────────────────────────────────────────────

/// Returns the full instance catalog for a cloud provider.
pub fn instance_catalog(provider: CloudProvider) -> Vec<InstanceFamily> {
    match provider {
        CloudProvider::Aws => aws_catalog(),
        CloudProvider::Gcp => gcp_catalog(),
        CloudProvider::Azure => azure_catalog(),
        CloudProvider::OnPrem => on_prem_catalog(),
    }
}

fn aws_catalog() -> Vec<InstanceFamily> {
    vec![
        InstanceFamily {
            provider: CloudProvider::Aws,
            family: "p5".to_string(),
            description: "NVIDIA H100 Tensor Core — highest GPU performance".to_string(),
            accelerator_type: AcceleratorType::Gpu,
            instance_types: vec![InstanceType {
                name: "p5.48xlarge".to_string(),
                vcpus: 192,
                memory_gb: 2048,
                accelerator: Some(AcceleratorDetail {
                    accelerator_type: AcceleratorType::Gpu,
                    device_name: "NVIDIA H100 80GB".to_string(),
                    count: 8,
                    memory_gb: Some(80),
                    compute_capability: Some("9.0".to_string()),
                    supports_mig: true,
                    mig_profiles: vec![
                        "1g.10gb".to_string(),
                        "2g.20gb".to_string(),
                        "3g.40gb".to_string(),
                        "7g.80gb".to_string(),
                    ],
                }),
                network_gbps: Some(3200.0),
                price_per_hour_usd: Some(98.32),
                k8s_resource_name: Some("nvidia.com/gpu".to_string()),
            }],
        },
        InstanceFamily {
            provider: CloudProvider::Aws,
            family: "p4d".to_string(),
            description: "NVIDIA A100 Tensor Core — large-scale GPU compute".to_string(),
            accelerator_type: AcceleratorType::Gpu,
            instance_types: vec![InstanceType {
                name: "p4d.24xlarge".to_string(),
                vcpus: 96,
                memory_gb: 1152,
                accelerator: Some(AcceleratorDetail {
                    accelerator_type: AcceleratorType::Gpu,
                    device_name: "NVIDIA A100 40GB".to_string(),
                    count: 8,
                    memory_gb: Some(40),
                    compute_capability: Some("8.0".to_string()),
                    supports_mig: true,
                    mig_profiles: vec![
                        "1g.5gb".to_string(),
                        "2g.10gb".to_string(),
                        "3g.20gb".to_string(),
                        "7g.40gb".to_string(),
                    ],
                }),
                network_gbps: Some(400.0),
                price_per_hour_usd: Some(32.77),
                k8s_resource_name: Some("nvidia.com/gpu".to_string()),
            }],
        },
        InstanceFamily {
            provider: CloudProvider::Aws,
            family: "g6".to_string(),
            description: "NVIDIA L4 — cost-effective GPU inference".to_string(),
            accelerator_type: AcceleratorType::Gpu,
            instance_types: vec![
                InstanceType {
                    name: "g6.xlarge".to_string(),
                    vcpus: 4,
                    memory_gb: 16,
                    accelerator: Some(AcceleratorDetail {
                        accelerator_type: AcceleratorType::Gpu,
                        device_name: "NVIDIA L4 24GB".to_string(),
                        count: 1,
                        memory_gb: Some(24),
                        compute_capability: Some("8.9".to_string()),
                        supports_mig: false,
                        mig_profiles: vec![],
                    }),
                    network_gbps: Some(10.0),
                    price_per_hour_usd: Some(0.805),
                    k8s_resource_name: Some("nvidia.com/gpu".to_string()),
                },
                InstanceType {
                    name: "g6.12xlarge".to_string(),
                    vcpus: 48,
                    memory_gb: 192,
                    accelerator: Some(AcceleratorDetail {
                        accelerator_type: AcceleratorType::Gpu,
                        device_name: "NVIDIA L4 24GB".to_string(),
                        count: 4,
                        memory_gb: Some(24),
                        compute_capability: Some("8.9".to_string()),
                        supports_mig: false,
                        mig_profiles: vec![],
                    }),
                    network_gbps: Some(40.0),
                    price_per_hour_usd: Some(4.602),
                    k8s_resource_name: Some("nvidia.com/gpu".to_string()),
                },
            ],
        },
        InstanceFamily {
            provider: CloudProvider::Aws,
            family: "g5".to_string(),
            description: "NVIDIA A10G — mixed GPU workloads".to_string(),
            accelerator_type: AcceleratorType::Gpu,
            instance_types: vec![
                InstanceType {
                    name: "g5.xlarge".to_string(),
                    vcpus: 4,
                    memory_gb: 16,
                    accelerator: Some(AcceleratorDetail {
                        accelerator_type: AcceleratorType::Gpu,
                        device_name: "NVIDIA A10G 24GB".to_string(),
                        count: 1,
                        memory_gb: Some(24),
                        compute_capability: Some("8.6".to_string()),
                        supports_mig: false,
                        mig_profiles: vec![],
                    }),
                    network_gbps: Some(10.0),
                    price_per_hour_usd: Some(1.006),
                    k8s_resource_name: Some("nvidia.com/gpu".to_string()),
                },
                InstanceType {
                    name: "g5.12xlarge".to_string(),
                    vcpus: 48,
                    memory_gb: 192,
                    accelerator: Some(AcceleratorDetail {
                        accelerator_type: AcceleratorType::Gpu,
                        device_name: "NVIDIA A10G 24GB".to_string(),
                        count: 4,
                        memory_gb: Some(24),
                        compute_capability: Some("8.6".to_string()),
                        supports_mig: false,
                        mig_profiles: vec![],
                    }),
                    network_gbps: Some(40.0),
                    price_per_hour_usd: Some(5.672),
                    k8s_resource_name: Some("nvidia.com/gpu".to_string()),
                },
            ],
        },
        InstanceFamily {
            provider: CloudProvider::Aws,
            family: "f2".to_string(),
            description: "Xilinx FPGA — custom hardware acceleration".to_string(),
            accelerator_type: AcceleratorType::Fpga,
            instance_types: vec![InstanceType {
                name: "f2.6xlarge".to_string(),
                vcpus: 24,
                memory_gb: 192,
                accelerator: Some(AcceleratorDetail {
                    accelerator_type: AcceleratorType::Fpga,
                    device_name: "Xilinx Alveo U250".to_string(),
                    count: 2,
                    memory_gb: Some(64),
                    compute_capability: None,
                    supports_mig: false,
                    mig_profiles: vec![],
                }),
                network_gbps: Some(25.0),
                price_per_hour_usd: Some(9.90),
                k8s_resource_name: Some("xilinx.com/fpga-u250".to_string()),
            }],
        },
        InstanceFamily {
            provider: CloudProvider::Aws,
            family: "inf2".to_string(),
            description: "AWS Inferentia2 — NPU for ML inference".to_string(),
            accelerator_type: AcceleratorType::Npu,
            instance_types: vec![
                InstanceType {
                    name: "inf2.xlarge".to_string(),
                    vcpus: 4,
                    memory_gb: 16,
                    accelerator: Some(AcceleratorDetail {
                        accelerator_type: AcceleratorType::Npu,
                        device_name: "AWS Inferentia2".to_string(),
                        count: 1,
                        memory_gb: Some(32),
                        compute_capability: None,
                        supports_mig: false,
                        mig_profiles: vec![],
                    }),
                    network_gbps: Some(15.0),
                    price_per_hour_usd: Some(0.758),
                    k8s_resource_name: Some("aws.amazon.com/neuroncore".to_string()),
                },
                InstanceType {
                    name: "inf2.24xlarge".to_string(),
                    vcpus: 96,
                    memory_gb: 384,
                    accelerator: Some(AcceleratorDetail {
                        accelerator_type: AcceleratorType::Npu,
                        device_name: "AWS Inferentia2".to_string(),
                        count: 6,
                        memory_gb: Some(32),
                        compute_capability: None,
                        supports_mig: false,
                        mig_profiles: vec![],
                    }),
                    network_gbps: Some(50.0),
                    price_per_hour_usd: Some(6.49),
                    k8s_resource_name: Some("aws.amazon.com/neuroncore".to_string()),
                },
            ],
        },
        InstanceFamily {
            provider: CloudProvider::Aws,
            family: "trn1".to_string(),
            description: "AWS Trainium — ML training acceleration".to_string(),
            accelerator_type: AcceleratorType::Npu,
            instance_types: vec![InstanceType {
                name: "trn1.32xlarge".to_string(),
                vcpus: 128,
                memory_gb: 512,
                accelerator: Some(AcceleratorDetail {
                    accelerator_type: AcceleratorType::Npu,
                    device_name: "AWS Trainium".to_string(),
                    count: 16,
                    memory_gb: Some(32),
                    compute_capability: None,
                    supports_mig: false,
                    mig_profiles: vec![],
                }),
                network_gbps: Some(800.0),
                price_per_hour_usd: Some(21.50),
                k8s_resource_name: Some("aws.amazon.com/neuroncore".to_string()),
            }],
        },
    ]
}

fn gcp_catalog() -> Vec<InstanceFamily> {
    vec![
        InstanceFamily {
            provider: CloudProvider::Gcp,
            family: "a3-highgpu".to_string(),
            description: "NVIDIA H100 — highest GPU compute on GCP".to_string(),
            accelerator_type: AcceleratorType::Gpu,
            instance_types: vec![InstanceType {
                name: "a3-highgpu-8g".to_string(),
                vcpus: 208,
                memory_gb: 1872,
                accelerator: Some(AcceleratorDetail {
                    accelerator_type: AcceleratorType::Gpu,
                    device_name: "NVIDIA H100 80GB".to_string(),
                    count: 8,
                    memory_gb: Some(80),
                    compute_capability: Some("9.0".to_string()),
                    supports_mig: true,
                    mig_profiles: vec![
                        "1g.10gb".to_string(),
                        "2g.20gb".to_string(),
                        "3g.40gb".to_string(),
                        "7g.80gb".to_string(),
                    ],
                }),
                network_gbps: Some(1600.0),
                price_per_hour_usd: Some(98.82),
                k8s_resource_name: Some("nvidia.com/gpu".to_string()),
            }],
        },
        InstanceFamily {
            provider: CloudProvider::Gcp,
            family: "a2-highgpu".to_string(),
            description: "NVIDIA A100 — general-purpose GPU compute".to_string(),
            accelerator_type: AcceleratorType::Gpu,
            instance_types: vec![
                InstanceType {
                    name: "a2-highgpu-1g".to_string(),
                    vcpus: 12,
                    memory_gb: 85,
                    accelerator: Some(AcceleratorDetail {
                        accelerator_type: AcceleratorType::Gpu,
                        device_name: "NVIDIA A100 40GB".to_string(),
                        count: 1,
                        memory_gb: Some(40),
                        compute_capability: Some("8.0".to_string()),
                        supports_mig: true,
                        mig_profiles: vec![
                            "1g.5gb".to_string(),
                            "2g.10gb".to_string(),
                            "3g.20gb".to_string(),
                        ],
                    }),
                    network_gbps: Some(24.0),
                    price_per_hour_usd: Some(3.67),
                    k8s_resource_name: Some("nvidia.com/gpu".to_string()),
                },
                InstanceType {
                    name: "a2-highgpu-4g".to_string(),
                    vcpus: 48,
                    memory_gb: 340,
                    accelerator: Some(AcceleratorDetail {
                        accelerator_type: AcceleratorType::Gpu,
                        device_name: "NVIDIA A100 40GB".to_string(),
                        count: 4,
                        memory_gb: Some(40),
                        compute_capability: Some("8.0".to_string()),
                        supports_mig: true,
                        mig_profiles: vec![
                            "1g.5gb".to_string(),
                            "2g.10gb".to_string(),
                            "3g.20gb".to_string(),
                        ],
                    }),
                    network_gbps: Some(50.0),
                    price_per_hour_usd: Some(14.69),
                    k8s_resource_name: Some("nvidia.com/gpu".to_string()),
                },
            ],
        },
        InstanceFamily {
            provider: CloudProvider::Gcp,
            family: "g2-standard".to_string(),
            description: "NVIDIA L4 — cost-effective inference on GCP".to_string(),
            accelerator_type: AcceleratorType::Gpu,
            instance_types: vec![
                InstanceType {
                    name: "g2-standard-4".to_string(),
                    vcpus: 4,
                    memory_gb: 16,
                    accelerator: Some(AcceleratorDetail {
                        accelerator_type: AcceleratorType::Gpu,
                        device_name: "NVIDIA L4 24GB".to_string(),
                        count: 1,
                        memory_gb: Some(24),
                        compute_capability: Some("8.9".to_string()),
                        supports_mig: false,
                        mig_profiles: vec![],
                    }),
                    network_gbps: Some(10.0),
                    price_per_hour_usd: Some(0.84),
                    k8s_resource_name: Some("nvidia.com/gpu".to_string()),
                },
                InstanceType {
                    name: "g2-standard-48".to_string(),
                    vcpus: 48,
                    memory_gb: 192,
                    accelerator: Some(AcceleratorDetail {
                        accelerator_type: AcceleratorType::Gpu,
                        device_name: "NVIDIA L4 24GB".to_string(),
                        count: 4,
                        memory_gb: Some(24),
                        compute_capability: Some("8.9".to_string()),
                        supports_mig: false,
                        mig_profiles: vec![],
                    }),
                    network_gbps: Some(32.0),
                    price_per_hour_usd: Some(5.21),
                    k8s_resource_name: Some("nvidia.com/gpu".to_string()),
                },
            ],
        },
        InstanceFamily {
            provider: CloudProvider::Gcp,
            family: "TPU v5e".to_string(),
            description: "Google TPU v5e — cost-effective ML inference".to_string(),
            accelerator_type: AcceleratorType::Tpu,
            instance_types: vec![
                InstanceType {
                    name: "ct5lp-hightpu-1t".to_string(),
                    vcpus: 24,
                    memory_gb: 48,
                    accelerator: Some(AcceleratorDetail {
                        accelerator_type: AcceleratorType::Tpu,
                        device_name: "TPU v5e".to_string(),
                        count: 1,
                        memory_gb: Some(16),
                        compute_capability: None,
                        supports_mig: false,
                        mig_profiles: vec![],
                    }),
                    network_gbps: Some(100.0),
                    price_per_hour_usd: Some(1.20),
                    k8s_resource_name: Some("google.com/tpu".to_string()),
                },
                InstanceType {
                    name: "ct5lp-hightpu-4t".to_string(),
                    vcpus: 120,
                    memory_gb: 192,
                    accelerator: Some(AcceleratorDetail {
                        accelerator_type: AcceleratorType::Tpu,
                        device_name: "TPU v5e".to_string(),
                        count: 4,
                        memory_gb: Some(16),
                        compute_capability: None,
                        supports_mig: false,
                        mig_profiles: vec![],
                    }),
                    network_gbps: Some(200.0),
                    price_per_hour_usd: Some(4.80),
                    k8s_resource_name: Some("google.com/tpu".to_string()),
                },
            ],
        },
        InstanceFamily {
            provider: CloudProvider::Gcp,
            family: "TPU v5p".to_string(),
            description: "Google TPU v5p — high-performance ML training".to_string(),
            accelerator_type: AcceleratorType::Tpu,
            instance_types: vec![InstanceType {
                name: "ct5p-hightpu-4t".to_string(),
                vcpus: 208,
                memory_gb: 448,
                accelerator: Some(AcceleratorDetail {
                    accelerator_type: AcceleratorType::Tpu,
                    device_name: "TPU v5p".to_string(),
                    count: 4,
                    memory_gb: Some(95),
                    compute_capability: None,
                    supports_mig: false,
                    mig_profiles: vec![],
                }),
                network_gbps: Some(400.0),
                price_per_hour_usd: Some(16.80),
                k8s_resource_name: Some("google.com/tpu".to_string()),
            }],
        },
    ]
}

fn azure_catalog() -> Vec<InstanceFamily> {
    vec![
        InstanceFamily {
            provider: CloudProvider::Azure,
            family: "ND H100 v5".to_string(),
            description: "NVIDIA H100 — Azure's highest GPU compute".to_string(),
            accelerator_type: AcceleratorType::Gpu,
            instance_types: vec![InstanceType {
                name: "Standard_ND96isr_H100_v5".to_string(),
                vcpus: 96,
                memory_gb: 1900,
                accelerator: Some(AcceleratorDetail {
                    accelerator_type: AcceleratorType::Gpu,
                    device_name: "NVIDIA H100 80GB".to_string(),
                    count: 8,
                    memory_gb: Some(80),
                    compute_capability: Some("9.0".to_string()),
                    supports_mig: true,
                    mig_profiles: vec![
                        "1g.10gb".to_string(),
                        "2g.20gb".to_string(),
                        "3g.40gb".to_string(),
                        "7g.80gb".to_string(),
                    ],
                }),
                network_gbps: Some(3200.0),
                price_per_hour_usd: Some(98.32),
                k8s_resource_name: Some("nvidia.com/gpu".to_string()),
            }],
        },
        InstanceFamily {
            provider: CloudProvider::Azure,
            family: "NC A100 v4".to_string(),
            description: "NVIDIA A100 — general GPU compute on Azure".to_string(),
            accelerator_type: AcceleratorType::Gpu,
            instance_types: vec![
                InstanceType {
                    name: "Standard_NC24ads_A100_v4".to_string(),
                    vcpus: 24,
                    memory_gb: 220,
                    accelerator: Some(AcceleratorDetail {
                        accelerator_type: AcceleratorType::Gpu,
                        device_name: "NVIDIA A100 80GB".to_string(),
                        count: 1,
                        memory_gb: Some(80),
                        compute_capability: Some("8.0".to_string()),
                        supports_mig: true,
                        mig_profiles: vec![
                            "1g.10gb".to_string(),
                            "2g.20gb".to_string(),
                            "3g.40gb".to_string(),
                        ],
                    }),
                    network_gbps: Some(12.5),
                    price_per_hour_usd: Some(3.67),
                    k8s_resource_name: Some("nvidia.com/gpu".to_string()),
                },
                InstanceType {
                    name: "Standard_NC96ads_A100_v4".to_string(),
                    vcpus: 96,
                    memory_gb: 880,
                    accelerator: Some(AcceleratorDetail {
                        accelerator_type: AcceleratorType::Gpu,
                        device_name: "NVIDIA A100 80GB".to_string(),
                        count: 4,
                        memory_gb: Some(80),
                        compute_capability: Some("8.0".to_string()),
                        supports_mig: true,
                        mig_profiles: vec![
                            "1g.10gb".to_string(),
                            "2g.20gb".to_string(),
                            "3g.40gb".to_string(),
                        ],
                    }),
                    network_gbps: Some(40.0),
                    price_per_hour_usd: Some(14.69),
                    k8s_resource_name: Some("nvidia.com/gpu".to_string()),
                },
            ],
        },
        InstanceFamily {
            provider: CloudProvider::Azure,
            family: "NC T4 v3".to_string(),
            description: "NVIDIA T4 — budget GPU for inference".to_string(),
            accelerator_type: AcceleratorType::Gpu,
            instance_types: vec![InstanceType {
                name: "Standard_NC4as_T4_v3".to_string(),
                vcpus: 4,
                memory_gb: 28,
                accelerator: Some(AcceleratorDetail {
                    accelerator_type: AcceleratorType::Gpu,
                    device_name: "NVIDIA T4 16GB".to_string(),
                    count: 1,
                    memory_gb: Some(16),
                    compute_capability: Some("7.5".to_string()),
                    supports_mig: false,
                    mig_profiles: vec![],
                }),
                network_gbps: Some(8.0),
                price_per_hour_usd: Some(0.526),
                k8s_resource_name: Some("nvidia.com/gpu".to_string()),
            }],
        },
        InstanceFamily {
            provider: CloudProvider::Azure,
            family: "NP v2".to_string(),
            description: "Xilinx Alveo U250 FPGA — custom acceleration".to_string(),
            accelerator_type: AcceleratorType::Fpga,
            instance_types: vec![InstanceType {
                name: "Standard_NP40s_v2".to_string(),
                vcpus: 40,
                memory_gb: 672,
                accelerator: Some(AcceleratorDetail {
                    accelerator_type: AcceleratorType::Fpga,
                    device_name: "Xilinx Alveo U250".to_string(),
                    count: 2,
                    memory_gb: Some(64),
                    compute_capability: None,
                    supports_mig: false,
                    mig_profiles: vec![],
                }),
                network_gbps: Some(20.0),
                price_per_hour_usd: Some(11.22),
                k8s_resource_name: Some("xilinx.com/fpga-u250".to_string()),
            }],
        },
    ]
}

fn on_prem_catalog() -> Vec<InstanceFamily> {
    vec![InstanceFamily {
        provider: CloudProvider::OnPrem,
        family: "Custom".to_string(),
        description: "On-premises nodes with manually configured accelerators".to_string(),
        accelerator_type: AcceleratorType::None,
        instance_types: vec![InstanceType {
            name: "custom-node".to_string(),
            vcpus: 0,
            memory_gb: 0,
            accelerator: None,
            network_gbps: None,
            price_per_hour_usd: None,
            k8s_resource_name: None,
        }],
    }]
}

// ─── CRD YAML Generation ────────────────────────────────────────────────────

impl DeploymentConfig {
    /// Generate a MegaDB CRD YAML snippet from this deployment configuration.
    pub fn to_crd_yaml(&self) -> String {
        let mut yaml = format!(
            r#"apiVersion: megadb.io/v1alpha1
kind: MegaDB
metadata:
  name: {}
spec:
  replicas: {}
  version: "latest"
  storage:
    storage_type: {}
"#,
            self.cluster_name,
            self.cpu_worker_pool.replicas,
            match self.storage_backend {
                StorageBackend::S3 => "s3",
                StorageBackend::Gcs => "gcs",
                StorageBackend::AzureBlob => "azure",
                StorageBackend::Local => "local",
            }
        );

        if let Some(ref gpu_pool) = self.gpu_worker_pool {
            yaml.push_str("  gpu_workers:\n");
            yaml.push_str(&format!("    replicas: {}\n", gpu_pool.replicas));
            if let Some(ref accel) = gpu_pool.accelerator {
                yaml.push_str("    accelerators:\n");
                yaml.push_str("      gpu:\n");
                yaml.push_str(&format!("        count: {}\n", accel.count_per_pod));
                yaml.push_str(&format!("        product: \"{}\"\n", accel.device_name));
                if let Some(ref mig) = accel.mig_profile {
                    yaml.push_str(&format!("        mig_profile: \"{mig}\"\n"));
                }
            }
            if !gpu_pool.node_selector.is_empty() {
                yaml.push_str("    node_selector:\n");
                for (k, v) in &gpu_pool.node_selector {
                    yaml.push_str(&format!("      {k}: \"{v}\"\n"));
                }
            }
            if !gpu_pool.tolerations.is_empty() {
                yaml.push_str("    tolerations:\n");
                for t in &gpu_pool.tolerations {
                    yaml.push_str(&format!("      - key: \"{}\"\n", t.key));
                    yaml.push_str(&format!("        operator: \"{}\"\n", t.operator));
                    if let Some(ref v) = t.value {
                        yaml.push_str(&format!("        value: \"{v}\"\n"));
                    }
                    yaml.push_str(&format!("        effect: \"{}\"\n", t.effect));
                }
            }
        }

        if let Some(ref fpga_pool) = self.fpga_worker_pool {
            if let Some(ref accel) = fpga_pool.accelerator {
                yaml.push_str("  accelerators:\n");
                yaml.push_str("    fpga:\n");
                yaml.push_str(&format!(
                    "      resource_name: \"{}\"\n",
                    accel.k8s_resource_name
                ));
                yaml.push_str(&format!("      count: {}\n", accel.count_per_pod));
            }
        }

        if self.keda_enabled {
            yaml.push_str("  # KEDA autoscaling (configure ScaledObject separately)\n");
            yaml.push_str(&format!(
                "  # min_replicas: {}\n",
                self.cpu_worker_pool.min_replicas
            ));
            yaml.push_str(&format!(
                "  # max_replicas: {}\n",
                self.cpu_worker_pool.max_replicas
            ));
        }

        yaml
    }
}

// ─── Tests ──────────────────────────────────────────────────────────────────

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn cloud_provider_labels() {
        assert_eq!(CloudProvider::Aws.label(), "AWS");
        assert_eq!(CloudProvider::Gcp.label(), "GCP");
        assert_eq!(CloudProvider::Azure.label(), "Azure");
        assert_eq!(CloudProvider::OnPrem.label(), "On-Premises");
    }

    #[test]
    fn accelerator_type_labels() {
        assert_eq!(AcceleratorType::Gpu.label(), "GPU");
        assert_eq!(AcceleratorType::Fpga.label(), "FPGA");
        assert_eq!(AcceleratorType::Tpu.label(), "TPU");
        assert_eq!(AcceleratorType::Npu.label(), "NPU");
    }

    #[test]
    fn storage_backend_defaults() {
        assert_eq!(
            StorageBackend::default_for(CloudProvider::Aws),
            StorageBackend::S3
        );
        assert_eq!(
            StorageBackend::default_for(CloudProvider::Gcp),
            StorageBackend::Gcs
        );
        assert_eq!(
            StorageBackend::default_for(CloudProvider::Azure),
            StorageBackend::AzureBlob
        );
        assert_eq!(
            StorageBackend::default_for(CloudProvider::OnPrem),
            StorageBackend::Local
        );
    }

    #[test]
    fn aws_catalog_has_gpu_fpga_npu() {
        let catalog = instance_catalog(CloudProvider::Aws);
        let types: Vec<AcceleratorType> = catalog.iter().map(|f| f.accelerator_type).collect();
        assert!(types.contains(&AcceleratorType::Gpu));
        assert!(types.contains(&AcceleratorType::Fpga));
        assert!(types.contains(&AcceleratorType::Npu));
    }

    #[test]
    fn gcp_catalog_has_gpu_tpu() {
        let catalog = instance_catalog(CloudProvider::Gcp);
        let types: Vec<AcceleratorType> = catalog.iter().map(|f| f.accelerator_type).collect();
        assert!(types.contains(&AcceleratorType::Gpu));
        assert!(types.contains(&AcceleratorType::Tpu));
    }

    #[test]
    fn azure_catalog_has_gpu_fpga() {
        let catalog = instance_catalog(CloudProvider::Azure);
        let types: Vec<AcceleratorType> = catalog.iter().map(|f| f.accelerator_type).collect();
        assert!(types.contains(&AcceleratorType::Gpu));
        assert!(types.contains(&AcceleratorType::Fpga));
    }

    #[test]
    fn crd_yaml_generation() {
        let config = DeploymentConfig {
            cluster_name: "megadb-prod".to_string(),
            provider: CloudProvider::Aws,
            cpu_worker_pool: WorkerPoolConfig {
                instance_type: "m6i.4xlarge".to_string(),
                replicas: 3,
                min_replicas: 2,
                max_replicas: 10,
                accelerator: None,
                node_selector: vec![],
                tolerations: vec![],
            },
            gpu_worker_pool: Some(WorkerPoolConfig {
                instance_type: "g6.12xlarge".to_string(),
                replicas: 2,
                min_replicas: 1,
                max_replicas: 8,
                accelerator: Some(PoolAccelerator {
                    accelerator_type: AcceleratorType::Gpu,
                    k8s_resource_name: "nvidia.com/gpu".to_string(),
                    count_per_pod: 4,
                    device_name: "NVIDIA L4 24GB".to_string(),
                    mig_profile: None,
                }),
                node_selector: vec![(
                    "cloud.google.com/gke-accelerator".to_string(),
                    "nvidia-l4".to_string(),
                )],
                tolerations: vec![TolerationConfig {
                    key: "nvidia.com/gpu".to_string(),
                    operator: "Exists".to_string(),
                    value: None,
                    effect: "NoSchedule".to_string(),
                }],
            }),
            fpga_worker_pool: None,
            npu_worker_pool: None,
            storage_backend: StorageBackend::S3,
            keda_enabled: true,
            estimated_monthly_cost_usd: 15000.0,
        };

        let yaml = config.to_crd_yaml();
        assert!(yaml.contains("megadb-prod"));
        assert!(yaml.contains("replicas: 3"));
        assert!(yaml.contains("gpu_workers:"));
        assert!(yaml.contains("NVIDIA L4 24GB"));
        assert!(yaml.contains("nvidia.com/gpu"));
        assert!(yaml.contains("storage_type: s3"));
    }

    #[test]
    fn serde_roundtrip() {
        let node = NodeHardware {
            node_name: "gpu-node-0".to_string(),
            provider_instance_type: Some("p4d.24xlarge".to_string()),
            cpu_allocatable: "96".to_string(),
            memory_allocatable_bytes: 1_152_000_000_000,
            accelerators: vec![DetectedAccelerator {
                resource_name: "nvidia.com/gpu".to_string(),
                accelerator_type: AcceleratorType::Gpu,
                allocatable: 8,
                allocated: 4,
                device_name: Some("NVIDIA A100 40GB".to_string()),
            }],
            labels: vec![(
                "node.kubernetes.io/instance-type".to_string(),
                "p4d.24xlarge".to_string(),
            )],
        };
        let json = serde_json::to_string(&node).unwrap();
        let decoded: NodeHardware = serde_json::from_str(&json).unwrap();
        assert_eq!(decoded.node_name, "gpu-node-0");
        assert_eq!(decoded.accelerators.len(), 1);
        assert_eq!(decoded.accelerators[0].allocatable, 8);
    }
}
